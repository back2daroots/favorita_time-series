# ğŸª Favorita Store Sales Forecasting

**Goal:**  
Build a time-series forecasting model to predict daily unit sales for each product family in each store, using historical sales, promotions, and calendar data.

## ğŸ·ï¸ Overview

This project tackles the CorporaciÃ³n Favorita Store Sales Forecasting challenge â€” predicting daily unit sales for thousands of product families across multiple Ecuadorian stores.
The goal was to build an end-to-end machine learning pipeline capable of modeling temporal dependencies, promotions, and holiday effects while maintaining scalability for millions of rows.

The final solution combines LightGBM and XGBoost models via simple weighted blending, achieving a holdout sMAPE of ~50.9, positioning the solution among the top-performing public submissions.


## ğŸ“Š Dataset

| Property | Description |
|-----------|--------------|
| **Source** | [Kaggle â€” Store Sales Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) |
| **Size** | ~3.0M records across 54 stores and 33 product families |
|Test size | ~28K rows (28 forecast days) |
| **Target Variable** | `sales` â€” daily unit sales for each (store, family) |

**Author:** back2daroots  
**Environment:** Python 3.11 (Conda, pip-tools), Linux/Mac  
**Core Libraries:** `pandas`, `numpy`, `scikit-learn`, `xgboost`, `optuna`, `yaml`, `matplotlib`, `seaborn`

---
## ğŸ“‚ Project Structure
```
store_sales/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # Feature, model, and path settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Original CSVs (train, test, oil, holidays, transactions)
â”‚   â””â”€â”€ processed/               # Processed and feature-enriched data
â”œâ”€â”€ models/                      # Trained model artifacts (.joblib)
â”œâ”€â”€ results/                     # Evaluation outputs and error diagnostics
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py                  # Data loading and preparation
â”‚   â”œâ”€â”€ features.py              # Feature generation and FE logic
â”‚   â”œâ”€â”€ models.py                # Model definitions and wrappers
â”‚   â”œâ”€â”€ metrics.py               # RMSE, MAE, sMAPE, CV utils
â”‚   â”œâ”€â”€ logging_utils.py         # Experiment logging
â”‚   â””â”€â”€ utils.py                 # Helpers, validation, config parsing
â”œâ”€â”€ plots/                       # Residuals, feature importance, etc
  
â”œâ”€â”€ cv_run.py                    # Cross-validation runner
â”œâ”€â”€ train.py                     # Train final model on full data
â”œâ”€â”€ analyze_errors.py            # Post-holdout error analysis
â”œâ”€â”€ experiments_log.csv          # Experiment registry
â”œâ”€â”€ predict_test.py              # Create a submission file
â”œâ”€â”€ blend_holdout_quick.py       # Execute blending
â”œâ”€â”€ environment.yml              # Environment specification
â””â”€â”€ .gitignore
```
---

### 1ï¸âƒ£ Feature Engineering
The pipeline builds features per `(store_nbr, family)` group:
- **Lags:** 1, 7, 14, 28, 56, 84 days  
- **Rolling statistics:** mean & std over 7, 28, 56 days  
- **Promo features:** current & lagged `onpromotion`, rolling sums & means  
- **Calendar features:** day-of-week, month, year, is_weekend, month start/end  
- **Holidays:** `is_holiday`, `is_preholiday_1`, `is_postholiday_1`  
- **Interactions:** `rmean_sales_7 Ã— onpromotion`, `is_friday Ã— month_end`

All features are computed **causally** (using `shift(1)`), ensuring no data leakage.

---

## ğŸ§  Model Training

Models are defined in `src/models.py` and configured in `configs/config.yaml`.

Models experimented with:
- `XGBoostRegressor`  (hist tree method, tuned with Optuna).
- `LightGBMRegressor` (fast histogram booster).
- `CatBoostRegressor` (baseline comparison).
- `Final blend` = 0.35 * LGBM + 0.65 * XGB

Hyperparameter tuning was performed using **Optuna** with a 5-fold **time-series CV** split.

## Validation
	â€¢	Date-based holdout split (last 28 days) + 4-fold time-based CV.
	â€¢	Metrics: RMSE, MAE, and sMAPE (primary ranking metric).

---

## ğŸ’¡ Key Insights
	â€¢	Combining short and long rolling windows (7/28/56 days) captured both weekly and monthly trends.
	â€¢	Promotion and holiday-related features notably improved generalization and reduced overfitting.
	â€¢	Hierarchical (store/family) rolling means stabilized model behavior for smaller sales segments.
	â€¢	Simple linear blending of XGB + LGBM reduced sMAPE by ~1.4 pp versus individual models.
	â€¢	Major residuals concentrated in Beverages, Grocery I, and Cleaning families and stores 44â€“47.

---

## ğŸ§ª Error Analysis (Holdout)

### ğŸ·ï¸ Top-5 Product Families (by MAE)
| Family     | MAE ($) |
|-------------|---------|
| BEVERAGES   | 104.21  |
| GROCERY I   | 100.18  |
| CLEANING    |  74.97  |
| PRODUCE     |  70.75  |
| DAIRY       |  36.72  |

### ğŸ¬ Top-4 Stores (by MAE)
| Store | MAE ($) |
|--------|----------|
| 44     |  50.41  |
| 47     |  45.55  |
| 45     |  40.71  |
| 46     |  37.70  |

### ğŸ“… Error by Day of Week
| DOW | MAE ($) |
|------|----------|
| 6 (Sunday)   | 24.63 |
| 5 (Saturday) | 24.44 |
| 4 (Friday)   | 20.86 |
| 0 (Monday)   | 20.82 |
| 3 (Thursday) | 17.94 |
| 1 (Tuesday)  | 17.93 |
| 2 (Wednesday)| 17.78 |

---
### Model Diagnostics

**Residuals vs Predicted**
![Residuals vs Predicted](plots/residuals_scatter_best.png)

**MAE by Product Family**
![MAE by Product Family](plots/mae_by_family.png)

**sMAPE by Day of Week**
![sMAPE by DOW](plots/smape_by_dow.png)

**Feature Importance (LGBM)**
![Feature Importance](plots/fi_lgbm_top20.png)


---
## ğŸ Final Results

| Model      | RMSE | MAE | sMAPE | Comment |
|-------------|------|-----|--------|----------|
| XGB         | 56.9 | 20.6 | 51.3 | Tuned baseline |
| LGBM        | 56.8 | 18.6 | 52.3 | Better alignment CV vs holdout |
| CatBoost    | 73.1 | 26.2 | 61.4 | Stable but lower |
| Blend (0.35 LGBM / 0.65 XGB) | â€” | â€” | **50.9** | Best submission |


## ğŸš€ How to Run

```bash
# 1. Activate environment
conda activate favorita-ts

# 2. Run CV tuning
python cv_run.py --config configs/config.yaml

# 3. Train final model
python train.py --config configs/config.yaml

# 4. Analyze errors
python analyze_errors.py --config configs/config.yaml
